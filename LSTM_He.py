# -*- coding: utf-8 -*-
"""Biohackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x1TwGVrYLKolJEQFuANAc6RqFiKKDv0_
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

X_train= pd.read_csv("X_train.csv")
X_test = pd.read_csv("X_test.csv")
y_train = pd.read_csv("y_train.csv")
y_test = pd.read_csv("y_test.csv")
X=pd.read_csv("X.csv")
y=pd.read_csv("y.csv")

"""# Full connected 2 layered RNN"""

class Data(Dataset):
    def __init__(self,train=True):
      if train:
        self.X=torch.from_numpy(X_train.drop("name",1).to_numpy())
        self.y=torch.from_numpy(y_train.iloc[:,0].to_numpy()[:,None])
      else:
        self.X=torch.from_numpy(X_test.drop("name",1).to_numpy())
        self.y=torch.from_numpy(y_test.iloc[:,0].to_numpy()[:,None])

    def __getitem__(self,index):
        return self.X[index],self.y[index]
        
    def __len__(self):
        return self.X.shape[0]

class regression2(nn.Module):
    
    def __init__(self,in_size,H1,out_size):
        super().__init__()
        self.linear1=nn.Linear(in_size,H1)
        self.linear2=nn.Linear(H1,out_size)
        self.activation=nn.LeakyReLU(0.2) 
    
    def forward(self,x):
        return self.linear2(self.activation(self.linear1(x)))

train_dat= Data()
val_dat= Data(train=False)

def train_model(model,epochs):
    train_loader=DataLoader(dataset=train_dat,batch_size=512)
    val_loader=DataLoader(dataset=val_dat,batch_size=7541)
    loss_fun=nn.L1Loss()
    optimizer=optim.Adam(model.parameters(),lr=0.01)
    
    TRAIN_LOSS=[]
    VAL_LOSS=[]

    for epoch in range(epochs):
        for x,y in train_loader:
            optimizer.zero_grad()
            z=model(x.float())
            loss=loss_fun(z.float(),y.float())
            loss.backward()
            optimizer.step()
            TRAIN_LOSS.append(loss.data.item())
            
        if (epoch+1)%20==0:
            for x,y in val_loader:
                z=model(x.float())
                loss=loss_fun(z.float(),y.float())
                print(epoch,loss)
                VAL_LOSS.append(loss)

    return TRAIN_LOSS,VAL_LOSS

model2=regression2(1032,128,1)
TRAIN_LOSS,VAL_LOSS=train_model(model2,100)

plt.plot(TRAIN_LOSS)
print(TRAIN_LOSS[-1])

# Compute correlation
y_pred = model2(val_dat.X.float()).detach().numpy()
plt.scatter(y_pred,y_test.iloc[:,0])
plt.xlabel("y_pred")
plt.ylabel("y_true")

df = pd.DataFrame({"y_true":y_test.iloc[:,0],"y_pred":y_pred.flatten()})
df.corr()

"""# Dummy model: always predict mean"""

train_mean=y_train.iloc[:,0].mean()
train_MSE =y_train.iloc[:,0].var()
train_MSE

SE=(y_test.iloc[:,0]-train_mean)**2
test_MSE=SE.mean()
test_MSE

"""# RNN"""

import tensorflow as tf

tf.keras.backend.clear_session()

model = tf.keras.models.Sequential([
  tf.keras.layers.SimpleRNN(43, input_shape=[43,24], return_sequences=True),
  tf.keras.layers.SimpleRNN(43),
  tf.keras.layers.Dense(1),
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss=tf.keras.losses.mean_squared_error,
              optimizer=optimizer)

history = model.fit(X_train.drop("name",1).to_numpy().reshape(30165,43,24),y_train.iloc[:,0].to_numpy(), epochs=50)

y_pred = model.predict(X_test.drop("name",1).to_numpy().reshape(7541,43,24))

SE=(y_pred-y_test)**2
SE.mean()

# Compute correlation
y_pred = model.predict(X_test.drop("name",1).to_numpy().reshape(7541,43,24))
plt.scatter(y_pred,y_test.iloc[:,0])
plt.xlabel("y_pred")
plt.ylabel("y_true")

df = pd.DataFrame({"y_true":y_test.iloc[:,0],"y_pred":y_pred.flatten()})
df.corr()

"""# 2 layer LSTM"""

tf.keras.backend.clear_session()

model_LSTM = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43, input_shape=[43,24], return_sequences=True)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43)),
  tf.keras.layers.Dense(1),
])


optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
model_LSTM.compile(loss=tf.keras.losses.mean_squared_error,
              optimizer=optimizer)
history = model_LSTM.fit(X_train.drop("name",1).to_numpy().reshape(30165,43,24).astype("float"),
                         y_train.iloc[:,0].to_numpy(), 
                         epochs=80)

y_pred = model_LSTM.predict(X_test.drop("name",1).to_numpy().reshape(7541,43,24).astype("float"))
SE=(y_pred-y_test)**2
SE.mean()

plt.scatter(y_pred,y_test.iloc[:,0])
plt.xlabel("y_pred")
plt.ylabel("y_true")

df = pd.DataFrame({"y_true":y_test.iloc[:,0],"y_pred":y_pred.flatten()})
df.corr()

"""# 2 Layer LSTM MAE loss"""

tf.keras.backend.clear_session()

LSTM2_mae = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43, input_shape=[43,24], return_sequences=True)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43)),
  tf.keras.layers.Dense(1),
])


optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
LSTM2_mae.compile(loss=tf.keras.losses.MeanAbsoluteError(),
              optimizer=optimizer)
history = LSTM2_mae.fit(X_train.drop("name",1).to_numpy().reshape(30165,43,24).astype("float"),
                         y_train.iloc[:,0].to_numpy(), 
                         epochs=80)

y_pred = LSTM2_mae.predict(X_test.drop("name",1).to_numpy().reshape(7541,43,24).astype("float"))
SE=(y_pred-y_test)**2
SE.mean()

plt.scatter(y_pred,y_test.iloc[:,0])
plt.xlabel("y_pred")
plt.ylabel("y_true")

df = pd.DataFrame({"y_true":y_test.iloc[:,0],"y_pred":y_pred.flatten()})
df.corr()

"""# One layer LSTM"""

tf.keras.backend.clear_session()

LSTM_mae = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43, input_shape=[43,24])),
  tf.keras.layers.Dense(1)
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
LSTM_mae.compile(loss=tf.keras.losses.MeanAbsoluteError(),
              optimizer=optimizer)
history = LSTM_mae.fit(X_train.drop("name",1).to_numpy().reshape(30165,43,24).astype("float"),
                         y_train.iloc[:,0].to_numpy(), 
                         epochs=80)

y_pred = model_LSTM1.predict(X_test.drop("name",1).to_numpy().reshape(7541,43,24).astype("float"))
SE=(y_pred-y_test)**2
SE.mean()

plt.scatter(y_pred,y_test.iloc[:,0])
plt.xlabel("y_pred")
plt.ylabel("y_true")

df = pd.DataFrame({"y_true":y_test.iloc[:,0],"y_pred":y_pred.flatten()})
df.corr()

"""# LSTM Mean absolute error"""

tf.keras.backend.clear_session()

LSTM1_mae = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43, input_shape=[43,24])),
  tf.keras.layers.Dense(1)
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
LSTM1_mae.compile(loss=tf.keras.losses.MeanAbsoluteError(),
              optimizer=optimizer)
history = LSTM1_mae.fit(X_train.drop("name",1).to_numpy().reshape(30165,43,24).astype("float"),
                         y_train.iloc[:,0].to_numpy(), 
                         epochs=80)

y_pred = LSTM1_mae.predict(X_test.drop("name",1).to_numpy().reshape(7541,43,24).astype("float"))
SE=(y_pred-y_test)**2
SE.mean()

plt.scatter(y_pred,y_test.iloc[:,0])
plt.xlabel("y_pred")
plt.ylabel("y_true")

df = pd.DataFrame({"y_true":y_test.iloc[:,0],"y_pred":y_pred.flatten()})
df.corr()

"""# Final model"""

tf.keras.backend.clear_session()

LSTM2_mae = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43, input_shape=[43,24], return_sequences=True)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43, input_shape=[43,24], return_sequences=True)),                                      
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(43, input_shape=[43,24])),
  tf.keras.layers.Dense(1),
])


optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
LSTM2_mae.compile(loss=tf.keras.losses.MeanAbsoluteError(),
              optimizer=optimizer)
history = LSTM2_mae.fit(X.drop("name",1).to_numpy().reshape(37706,43,24).astype("float"),
                         y.iloc[:,0].to_numpy(), 
                         epochs=80)

single_test_dat = pd.read_csv("twohot_single_test.csv")
single_test_dat.drop("name",1).shape

single_pred = LSTM2_mae.predict(single_test_dat.drop("name",1).
                                drop("stabilityscore",1).
                                to_numpy().reshape(2138,43,24).astype("float"))
single_pred

SE=(single_pred.flatten()-single_test_dat.loc[:,"stabilityscore"])**2
SE.mean()

plt.scatter(single_pred.flatten(),single_test_dat.loc[:,"stabilityscore"])

df = pd.DataFrame({"y_true":single_test_dat.loc[:,"stabilityscore"],"y_pred":single_pred.flatten()})
df.corr()

multi_test_dat = pd.read_csv("twohot_multi_test.csv")
multi_test_dat.drop("name",1).shape

multi_pred = LSTM2_mae.predict(multi_test_dat.drop("name",1).
                                drop("stabilityscore",1).
                                to_numpy().reshape(7290,43,24).astype("float"))
SE=(multi_pred.flatten()-multi_test_dat.loc[:,"stabilityscore"])**2
SE.mean()

plt.scatter(multi_pred.flatten(),multi_test_dat.loc[:,"stabilityscore"])

df = pd.DataFrame({"y_true":multi_test_dat.loc[:,"stabilityscore"],"y_pred":multi_pred.flatten()})
df.corr()

out_df = pd.DataFrame({"name":multi_test_dat.name,
                       "Stability_score":multi_test_dat.loc[:,"stabilityscore"],"Prediction":multi_pred.flatten()})
out_df.head()

out_df.to_csv("Multi_LSTM")



