# -*- coding: utf-8 -*-
"""encode&xgboost_feng.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qifrY1W-g4ntqTHNYqBF2g0GfKMaajnj
"""

import os
from google.colab import drive
drive.mount('/content/gdrive')
# change to relevant directory
root_path = "/content/gdrive/My Drive/Master/BIOH2021"  #自己的文件夹
os.chdir(root_path)
os.getcwd()

!pip install biopython
import pandas as pd
import numpy as np
from Bio.PDB.Polypeptide import PPBuilder
from Bio.Seq import Seq
from collections import Counter
import matplotlib.pyplot as plt

"""## input raw data"""

raw_single = pd.read_csv("single_muts_train.csv")
raw_single

raw_multi = pd.read_csv("multiple_muts_train.csv")
raw_multi

print(Counter(raw_single["sequence"].map(len)))
print(Counter(raw_multi["sequence"].map(len)))

"""## encode"""

from Bio.PDB.Polypeptide import PPBuilder
def onehot_encode_aa(sequence):
  aa_indices = []
  for aa in sequence:
    try:
      aa_index = Polypeptide.one_to_index(aa)
      #print(aa,"a")
    except:
      aa_index = 20
    aa_indices.append(aa_index)
  sequence_onehot = np.zeros((len(aa_indices),21))
  sequence_onehot[np.arange(len(aa_indices)),aa_indices]=1
  return sequence_onehot

def twohot(second):
  sed = []
  for i in second:
    if i == "T":
      sed_index = [0,0,1]
    if i == "E":
      sed_index = [0,1,0]
    if i == "H":
      sed_index = [1,0,0]
    sed.append(sed_index)
  return(sed)

onehot_single = []
for i in range(len(raw_single)):
  one_seq = []
  for site in range(43): 
    comb = list(onehot_encode_aa(raw_single["sequence"][i])[site]) + twohot(raw_single["secondary_structure"][i])[site]
    one_seq += comb 
  onehot_single.append(one_seq)

### encode train file

"""single"""

onehot_single = pd.DataFrame(onehot_single)
onehot_single["stabilityscore"] = raw_single["stabilityscore"]
onehot_single.index = raw_single["name"]
onehot_single #8550x1033

onehot_single.to_csv("twohot_single.csv")

"""multiple"""

onehot_multi = []
for i in range(len(raw_multi)):
  one_seq = []
  for site in range(43): 
    comb = list(onehot_encode_aa(raw_multi["sequence"][i])[site]) + twohot(raw_multi["secondary_structure"][i])[site]
    one_seq += comb 
  onehot_multi.append(one_seq)

onehot_multi = pd.DataFrame(onehot_multi)
onehot_multi["stabilityscore"] = raw_multi["stabilityscore"]
onehot_multi.index = raw_multi["name"]
onehot_multi = onehot_multi.dropna(axis = 0)  
onehot_multi #29156 x1033



onehot_multi.to_csv("twohot_multi.csv")

sum(onehot_multi.iloc[:,1032].isnull())
29187-31

"""### encode test file"""

raw_multi_test = pd.read_csv("multiple_muts_test.csv")
raw_single_test = pd.read_csv("single_muts_test.csv")

"""single test file"""

onehot_single_test = []
for i in range(len(raw_single_test)):
  one_seq = []
  for site in range(43): 
    comb = list(onehot_encode_aa(raw_single_test["sequence"][i])[site]) + twohot(raw_single_test["secondary_structure"][i])[site]
    one_seq += comb 
  onehot_single_test.append(one_seq)

onehot_single_test = pd.DataFrame(onehot_single_test)
onehot_single_test["stabilityscore"] = raw_single_test["stabilityscore"]
onehot_single_test.index = raw_single_test["name"]
onehot_single_test

onehot_single_test.to_csv("twohot_single_test.csv")

"""multiple test"""

onehot_multi_test = []
for i in range(len(raw_multi_test)):
  one_seq = []
  for site in range(43): 
    comb = list(onehot_encode_aa(raw_multi_test["sequence"][i])[site]) + twohot(raw_multi_test["secondary_structure"][i])[site]
    one_seq += comb 
  onehot_multi_test.append(one_seq)

onehot_multi_test = pd.DataFrame(onehot_multi)
onehot_multi_test["stabilityscore"] = raw_multi_test["stabilityscore"]
onehot_multi_test.index = raw_multi_test["name"]
onehot_multi_test = onehot_multi_test.dropna(axis = 0)  
onehot_multi_test

onehot_multi_test.to_csv("twohot_multi_test.csv")

"""## train 模板"""

# 注意这里只有single
X = onehot_single.iloc[:,0:1032]
Y = onehot_single.iloc[:,1032]

"""**!!!需要记录的结果 初始模型的mse结果,cv grid-search调参后的参数，调参后的mse结果**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

#初始模型的结果
knn = KNeighborsRegressor(n_neighbors= 5) #需要替换model
score_mse = cross_val_score(knn, X, Y, cv=5, scoring = "neg_mean_squared_error").mean()  #socring - r2
score_mse

#调参过程
from sklearn.model_selection import GridSearchCV
#这里填写各种要尝试的参数，可以在网上搜有啥
tuned_parameters = [{'n_neighbors':[1,3,5,7,9,11,13],
                     'weights':['uniform','distance'],
                     'algorithm' : ['auto', 'ball_tree',],
                     'metric' : ["euclidean"]}]


grid = GridSearchCV(KNeighborsRegressor(), #需要替换model
                    tuned_parameters, scoring= "neg_mean_squared_error", cv = 5)
grid.fit(Xtrain, Ytrain)
print("The best parameters are %s with a score of %f" % (grid.best_params_,grid.best_score_))

#CV grid-search 过程中每次参数跑出的结果---五次CV中的mean（mse）和std（mse）
means = grid.cv_results_['mean_test_score']
stds = grid.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, grid.cv_results_['params']):
  print("%f (+/-%0.03f) for %r" % (mean, std * 2, params))

#等有了test数据集，使用最优参数训练整个trian数据集，然后得到的模型用在test上，看结果score
………………
………………

#学习曲线 适合于单个参数的调整
f1_list = []
for i in range(1,1000,100): #小数 np.linscape()
  rfc = RandomForestClassifier(n_estimators=i,random_state=420)
  score = cross_val_score(rfc, X_train, y_train, cv=5, scoring = "neg_mean_squared_error").mean()
  f1_list.append(score)
print(max(f1_list),([*range(1,1000,100)][f1_list.index(max(f1_list))]))
plt.figure(figsize=[20,5])
plt.plot(range(1,1000,100),f1_list)
plt.show()

"""## train model

### input
"""

#input Data
twohot_train = pd.concat([onehot_single, onehot_multi])
twohot_train

X = twohot_train.iloc[:,0:1032]
Y = twohot_train.iloc[:,1032]

from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y,test_size=0.2,random_state=420)

"""### knn"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error as MSE

#初始模型的结果
knn = KNeighborsRegressor(n_neighbors= 5).fit(Xtrain,Ytrain)
MSE(Ytest,knn.predict(Xtest))

[*range(31,101,8)]

mse_list = []
for i in range(31,101,8):
  xgb = KNeighborsRegressor(n_neighbors= i).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*range(31,101,8)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot([*range(31,101,8)],mse_list)
plt.show()

knn = KNeighborsRegressor(n_neighbors= 63).fit(Xtrain,Ytrain)
MSE(Ytest,knn.predict(Xtest))

"""### xgboost"""

from xgboost import  XGBRegressor
from sklearn.metrics import mean_squared_error as MSE

xgb = XGBRegressor(n_estimators=100, objective= 'reg:squarederror').fit(Xtrain,Ytrain)
print("train", MSE(Ytrain,xgb.predict(Xtrain)))
print("test", MSE(Ytest,xgb.predict(Xtest)))

reg.feature_importances_

"""#### n_estimators"""

other_params = {
    'learning_rate': 0.1,  
    'max_depth': 5, 
    'min_child_weight': 1, 
    'subsample': 0.8, 
    'colsample_bytree': 0.8, 
    'gamma': 0,
    'reg_alpha': 0, 
    'reg_lambda': 1,
    #'objective': 'reg:squarederror',
    'nthread': 4,
    'scale_pos_weight': 1,
    'random_state': 42
    }

mse_list = []
for i in range(1,1000,100):
  xgb = XGBRegressor(n_estimators = i, **other_params).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*range(1,1000,100)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot(range(1,1000,100),mse_list)
plt.show()

mse_list = []
for i in range(150,300,10):
  xgb = XGBRegressor(n_estimators = i, **other_params).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*range(150,300,10)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot(range(150,300,10),mse_list)
plt.show()

"""#### learning_rate"""

other_params = {
    'n_estimators' : 280,
    #'learning_rate': 0.1,  
    'max_depth': 5, 
    'min_child_weight': 1, 
    'subsample': 0.8, 
    'colsample_bytree': 0.8, 
    'gamma': 0,
    'reg_alpha': 0, 
    'reg_lambda': 1,
    #'objective': 'reg:squarederror',
    'nthread': 4,
    'scale_pos_weight': 1,
    'random_state': 42
    }

mse_list = []
for i in np.linspace(0.01,0.2,10):
  xgb = XGBRegressor(learning_rate = i, **other_params).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*np.linspace(0.01,0.2,10)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot(np.linspace(0.01,0.2,10),mse_list)
plt.show()

"""#### max_depth"""

other_params = {
    'n_estimators' : 280,
    'learning_rate': 0.16,  
    #'max_depth': 5, 
    'min_child_weight': 1, 
    'subsample': 0.8, 
    'colsample_bytree': 0.8, 
    'gamma': 0,
    'reg_alpha': 0, 
    'reg_lambda': 1,
    #'objective': 'reg:squarederror',
    'nthread': 4,
    'scale_pos_weight': 1,
    'random_state': 42
    }

mse_list = []
for i in range(3,15,2):
  xgb = XGBRegressor(max_depth = i, **other_params).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*range(3,15,2)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot(range(3,15,2),mse_list)
plt.show()

mse_list = []
for i in range(15,30,5):
  xgb = XGBRegressor(max_depth = i, **other_params).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*range(15,30,5)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot(range(15,30,5),mse_list)
plt.show()

other_params = {
    'n_estimators' : 280,
    'learning_rate': 0.16,  
    'max_depth': 3, 
    #'min_child_weight': 1, 
    'subsample': 0.8, 
    'colsample_bytree': 0.8, 
    'gamma': 0,
    'reg_alpha': 0, 
    'reg_lambda': 1,
    #'objective': 'reg:squarederror',
    'nthread': 4,
    'scale_pos_weight': 1,
    'random_state': 42
    }

mse_list = []
for i in range(1,6,1):
  xgb = XGBRegressor(min_child_weight = i, **other_params).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*range(1,6,1)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot(range(1,6,1),mse_list)
plt.show()

"""#### gamma"""

other_params = {
    'n_estimators' : 280,
    'learning_rate': 0.16,  
    'max_depth': 3, 
    'min_child_weight': 1, 
    'subsample': 0.8, 
    'colsample_bytree': 0.8, 
    #'gamma': 0,
    'reg_alpha': 0, 
    'reg_lambda': 1,
    #'objective': 'reg:squarederror',
    'nthread': 4,
    'scale_pos_weight': 1,
    'random_state': 42
    }

mse_list = []
for i in np.linspace(0,1,11):
  xgb = XGBRegressor(gamma = i, **other_params).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*np.linspace(0,1,11)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot(np.linspace(0,1,11),mse_list)
plt.show()

"""#### subsample"""

other_params = {
    'n_estimators' : 280,
    'learning_rate': 0.16,  
    'max_depth': 3, 
    'min_child_weight': 0.2, 
    #'subsample': 0.8, 
    'colsample_bytree': 0.8, 
    'gamma': 0,
    'reg_alpha': 0, 
    'reg_lambda': 1,
    #'objective': 'reg:squarederror',
    'nthread': 4,
    'scale_pos_weight': 1,
    'random_state': 42
    }

mse_list = []
for i in np.linspace(0.3,1,8):
  xgb = XGBRegressor(subsample = i, **other_params).fit(Xtrain,Ytrain)
  mse_score = MSE(Ytest,xgb.predict(Xtest))
  mse_list.append(mse_score)
print(min(mse_list),([*np.linspace(0.3,1,8)][mse_list.index(min(mse_list))]))
plt.figure(figsize=[20,5])
plt.plot(np.linspace(0.3,1,8),mse_list)
plt.show()

"""### test"""

Xtrain = onehot_single.iloc[:,0:1032]
Ytrain = onehot_multi.iloc[:,1032]
Xtest_s = onehot_single_test.iloc[:,0:1032]
Ytest_s = onehot_single_test.iloc[:,1032]
Xtest_m = onehot_multi_test.iloc[:,0:1032]
Ytest_m = onehot_multi_test.iloc[:,1032]

params = {
    'n_estimators' : 280,
    'learning_rate': 0.16,  
    'max_depth': 3, 
    'min_child_weight': 0.2, 
    'subsample': 0.8, 
    'colsample_bytree': 0.8, 
    'gamma': 0,
    'reg_alpha': 0, 
    'reg_lambda': 1,
    #'objective': 'reg:squarederror',
    'nthread': 4,
    'scale_pos_weight': 1,
    'random_state': 42
    }

from xgboost import  XGBRegressor
from sklearn.metrics import mean_squared_error as MSE

xgb = XGBRegressor(**params).fit(Xtrain.values,Ytrain.values)
print("train", MSE(Ytrain.values,xgb.predict(Xtrain.values)))
print("test_single", MSE(Ytest_s.values,xgb.predict(Xtest_s.values)))
y_pred_s = xgb.predict(Xtest_s.values)

save_single = pd.DataFrame({"name":onehot_single.index,"stabilityscore":onehot_single["stabilityscore"], "y_pred":y_pred_s})
save_single = save_single.reset_index(drop=True)
save_single
save_single.to_csv("xgboost_single.csv")

print("test_multi", MSE(Ytest_m.values,xgb.predict(Xtest_m.values)))
y_pred_m = xgb.predict(Xtest_m.values)

save_multi = pd.DataFrame({"name":onehot_multi.index,"stabilityscore":onehot_multi["stabilityscore"], "y_pred":y_pred_m})
save_multi = save_multi.reset_index(drop=True)
save_multi

save_multi.to_csv("xgboost_multi.csv")

"""plot"""

import matplotlib.pyplot as plt
plt.scatter(y_pred_s,Ytest_s)
plt.xlabel("y_pred")
plt.ylabel("y_true")

df = pd.DataFrame({"y_true":Ytest_s,"y_pred":y_pred_s.flatten()})
df.corr()